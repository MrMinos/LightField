<!DOCTYPE html>
<html>
<head>
	<title>CS184 Final Project</title>
	<link rel="stylesheet" type="text/css" href="style.css">
</head>
<body>

<div class="container">
  <div class="row">
    <div class="col-sm-2">
     
    </div>
    <div class="col-sm-8">
      <center>

			<h1>Light Field Camera</h1>
			<h2>A Project Update and Pivot</h2>

      <br>

  			<h4>Minos Park</h4>
      		<h6>26631485</h6>
      		<h4>Bryan Perez</h4>
      		<h6>3032315274</h6>
      </center>

	  <br>
	  <img src="img/neuron2.png" style="width:50%">
	  <img src="img/neuron.jpg" style="width:49%">
	  Raw light field images of neurons
	

      <hr>
      <h3>Abstract</h3>
    	<p>
				Capturing 4D light field with plenoptic camera allows computational refocusing, computational correction of lens aberrations and more.
				In this project we wanted to extend Project 3-2 to implement the ability to refocus after the ray tracing is done, by having each pixel be a grid recording radiance from different directions.
				<br><br>
				Before pivoting, our proposal was a scene simulator. The proposal can be found <a href="previous.html">here</a>.
			</p><br><hr>
			<h3>Details on Pivot and Project's New Direction</h3>
			Slides and video can be found <a href="https://docs.google.com/presentation/d/e/2PACX-1vT0auI1OW4mDDVFGxp6guNDPCOQDpUmOA7jBlfm3JryxrW5vbEOQg1FT_GgR0VNDPIwsumGEnHaTWBQ/pub?start=false&loop=false&delayms=15000&slide=id.p">here</a>.
			<br>
			<h4>Why Pivot?</h4>
			The feedback we got from initial proposal was helpful in reckoning a few overlooked holes. Megan gave an outlook that the project seems "lack of moderate technical challenges."
			After the feedback was released, we discussed ways to better communicate in the proposal of our initial ideas in-depth to show the technical challenges.
			<br><br>
			After a few more discussion, we ultimately decided that it would be better if we pivot completely and work on ideas that were shown on the course website. Since by doing that, we can offload our worries about clarifying the depth of technical challenges and focus on making progress in the project, which by then we were running out of.
			<br><br>
			<em>Hence, this document serves as both our proposal and progress update.</em>
			<br><br>
			<h4>New Direction of The Project</h4> 
			<h5>- Abstract</h5>Please refer to the abstract above.
			<br>
			<h5>- Background</h5>
			The current ray tracer uses pin-hole or thin lens model which once the ray tracing is done, loses the ability to change the focus plane and cannot do after-the-fact change of aperture. The rendered scene is essentially a printed photo.
			If we implement a new type of lens called microlens on the previous model, which closely follows the Lytro camera model, then we can obtain pixels containing more data than the previous model. In turn, we can also implement a computational refocusing mechanism to get images that have different focus after the ray tracing is done.
			<br><br>
			We begin our project by extending Project 3-2 a thin-lens ray tracer. We will use the light field lens introduced in lecture as the model to implement in our camera.
			<br><h5>- Relevant papers/articles</h5>
			<ul>
				<li><a href="http://graphics.stanford.edu/papers/lfmicroscope/">Light Field Microscopy</a><br></li>
				<li><a href="http://graphics.stanford.edu/papers/lfmicroscope/levoy-lfmicroscope-sig06.pdf">- Paper: Light Field Microscopy</a><br></li>
				<li>- The above Light Field Microscopy webpage links to many other related resources.<br></li>
				<li><a href="https://graphics.stanford.edu/papers/lfcamera/lfcamera-150dpi.pdf">Light Field Photography with a Hand-held Plenoptic Camera</a><br></li>
				<li><a href="http://lightfield.stanford.edu/lfs.html">The (New) Stanford Light Field Archive</a><br><br></li>
			</ul>
			<h5>- Goals and Deliverables/Tasks</h5>
			<ul>
				While discussing how to approach the project, Professor Ng mentioned that we should try to do multiple "parts" that are all related to a topic -- in this case, light field. That would truly enable us to explore the subject in various ways.
				So each approach layed out here is less seqencial, dependent on each other and more a full project on its own. The expectation is that we will get at least one approach well completed and the rest of the approaches are excercises for our expansion of experience and satisfying the curiosity for the subject.
				
				<br><br>
				
				<li>First Approach -- LF data generation by the ray tracer from Project 3-2 with addition of a Lytro camera like microlens that was covered in the class</li>
				
				<ul>
					<li>Implementing microlens</li>
					<ul>
						<li>Last offering of this course had a project implementing a set of camera lens in our ray tracer system. Confer that project for how to implement microlens</li>
						<li>Have more details on the resolution of the microlens</li>
						<li>Check the rays progressing through the microlens, against what is expected.</li>
						<li>Save a screenshot of rays going through the microlens</li>
						<li>Confirm we get the somewhat expected raw data that is similar to the paper</li>
					</ul>
					<li>Computational refocusing mechanism</li>
					To use the gathered LF data, we can now work on the mechanism to enable refocusing and more
					<ul>
						<li>Start with something simple. As in lecture's example, try selecting just same (u,v) pixel from each data points and see how it does</li>
						<li>Following the paper listed above, implement the ability to choose the focus plane</li>
						<li>(optional, do this while exploring the second approach)<br>Make an interactive application that will take in the raw LF data and show various images.</li>
					</ul>
				</ul>

				<br>

				<li>Second Approach -- Use real world LF data and create a viewer that interactively shows the user various perspective of a captured image. </li>
				<ul>
					<li>The data is captured by arrays of DSLRs and u,v location of the camera is saved as metadata. We can use this to produce LF refocused image.</li>
					<li>Make a simulator that accepts LF images and produces interactive demo</li>
					<ul>
						<li>Similar tasks as the first approach's Computational refocusing mechanism</li>
						<li>- Start with something simple. As in lecture's example, try selecting just same (u,v) pixel from each data points and see how it does</li>
						<li>- Following the paper listed above, implement the ability to choose the focus plane</li>
					</ul>
				</ul>
			</ul>
			<h5>- Hope to deliver: stretched goal</h5>
			<ul>
				<li>We scheduled quite a lot for a single project. Although we hope to deliver all, it is likely that we may only finish one of the approaches laid out in the previous section</li>
			</ul>
			<br><hr>
			<h4>Prelim result and reflection on plan</h4>
			Initially the first approach was taken and yet the microlens implementation is proven to be hard. Unfortunately, the amount of work necessary to make such progress was not met. There is not much to exhibit because the product so far does not work. Naturally, there was more revision of the project.
			<br>Second approach will be taken during the final phase of the project. We will focus on implementing the computational refocusing mechanism and test the mechanism with the LF data available at Stanford Light Field Archive. As mentioned in the revised deliverable/tasks section, we plan to do what Professor Ng suggested.
			


			<br>
			<br>
			<br>
			<br>
			<br>
			<br>
			
		</div>
		<br>
		<br>

		<br>
		<br>

		<br>
		<br>

    <div class="col-sm-2">
      
    </div>
  </div>
</div>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
</body>
</html>
